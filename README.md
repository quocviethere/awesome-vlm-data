# Awesome VLM Data [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

This is the repository corresponding with the [blog post](https://quocviethere.github.io/blog/2025/vlmdata/) on data-centric analysis for Vision-Language Models.

## Table of Contents

- [Datasets](#datasets)
  - [Image-text pair data](#image-text-pair-data)
  - [Interleaved-text pair data](#interleaved-image-text-data)   

## Datasets
---
### Image-text pair data

1. **LXMERT (9.8M)** | Tan & Bansal (2019). LXMERT: Learning Cross-Modality Encoder Representations from Transformers. In *EMNLP*, 5099–5110. [PDF](https://doi.org/10.18653/v1/D19-1514)  

2. **UNITER (5.6M)** | Chen et al. (2020). UNITER: UNiversal Image-TExt Representation Learning. In *ECCV*, 104–120. [PDF](https://doi.org/10.1007/978-3-030-58577-8_7)  

3. **CLIP (400M)** | Radford et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. In *ICML*, 8748–8763. [PDF](https://doi.org/10.48550/arXiv.2103.00020)  

4. **CC3M** | Sharma et al. (2018). Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning. In *ACL*. [PDF](https://doi.org/10.18653/v1/P18-1238)  

5. **CC12M** | Changpinyo et al. (2021). Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts. In *CVPR*, 3558–3568. [PDF](https://doi.org/10.1109/CVPR46437.2021.00356)  

6. **WIT (37.5M)** | Srinivasan et al. (2021). WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning. In *SIGIR*, 2443–2449. [PDF](https://doi.org/10.1145/3404835.3463257)  

7. **ALIGN (1.3B samples)** | Jia et al. (2021). Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In *ICML*, 4904–4916.  

8. **ALBEF (5.1M)** | Li et al. (2021). Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. In *NeurIPS*, 9694–9705. [PDF](https://doi.org/10.48550/arXiv.2107.07651)  

9. **RedCaps (12M)** | Desai et al. (2021). RedCaps: Web-curated image-text data created by the people, for the people. In *NeurIPS*.  

10. **FILIP (300M)** | Yao et al. (2022). FILIP: Fine-grained Interactive Language-Image Pre-Training. In *ICLR*. [PDF](https://doi.org/10.48550/arXiv.2111.07783)  

11. **LAION-400M** | Schuhmann et al. (2021). LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. In *arXiv*.  

12. **FLD-900M** | Yuan et al. (2021). Florence: A New Foundation Model for Computer Vision. In *arXiv*.  

13. **PMD (70M)** | Singh et al. (2022). FLAVA: A Foundational Language And Vision Alignment Model. In *CVPR*, 15617–15629. [PDF](https://doi.org/10.1109/CVPR52688.2022.01519)  

14. **COYO-700M** | [GitHub Repository](https://github.com/kakaobrain/coyo-dataset)  

15. **LAION-5B** | Schuhmann et al. (2022). LAION-5B: An open large-scale dataset for training next-generation image-text models. In *NeurIPS*.  

16. **LiT (4B)** | Zhai et al. (2022). LiT: Zero-Shot Transfer with Locked-image text Tuning. In *CVPR*, 18102–18112. [PDF](https://doi.org/10.1109/CVPR52688.2022.01759)  

17. **BASIC (6B)** | Pham et al. (2023). Combined scaling for zero-shot transfer learning. In *Neurocomputing* (Vol. 555, p. 126658). [PDF](https://doi.org/10.1016/j.neucom.2023.126658)  

18. **WebLI (10B)** | Chen et al. (2022). PaLI: A Jointly-Scaled Multilingual Language-Image Model. In *ICLR*.  

---

### Interleaved Image-text data

> Dataset size measured in terms of number of tokens.

1. **CM3 (223B)** | Aghajanyan et al. (2022). CM3: A Causal Masked Multimodal Model of the Internet. In *arXiv*.  

2. **OBELICS (115B)** | Laurençon et al. (2023). OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents. In *NeurIPS*.  

3. **Multimodal-C4 (43B)** | Zhu et al. (2023). Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text. In *NeurIPS*. [PDF](https://doi.org/10.48550/arXiv.2304.06939)  

4. **mOSCAR (214B)** | Futeral et al. (2024). mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus. In *arXiv*.  

5. **Chameleon (400B)** | Team C. (2024). Chameleon: Mixed-Modal Early-Fusion Foundation Models. In *arXiv*.  

6. **MM1 (400B)** | McKinzie et al. (2025). MM1: Methods, Analysis and Insights from Multimodal LLM Pre-training. In *ECCV*, 304–323. [PDF](https://doi.org/10.1007/978-3-031-73397-0_18)  

7. **MINT-1T** | Awadalla et al. (2024). MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens. In *NeurIPS Datasets and Benchmarks Track*. [PDF](https://doi.org/10.48550/ARXIV.2406.11271)  

8. **OmniCorpus (1.7T)** | Li et al. (2024). OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text. In *arXiv*. [PDF](https://doi.org/10.48550/ARXIV.2406.08418)  

---

## Data Filtering

1. **MetaCLIP** | Xu et al. (2024). Demystifying CLIP Data. In *ICLR (Spotlight)*. [PDF]([https://doi.org/10.48550/ARXIV.2406.08418](https://openreview.net/pdf?id=5BCFlnfE1g))

2. Nguyen et al. (2022). Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP. In *NeurIPS*. [PDF]()

3. Fang et al. (2024). Data Filtering Networks. In *ICLR*. [PDF](https://doi.org/10.48550/arXiv.2309.17425)

4. Goyal et al. (2024). The Science of Data Filtering: Data Curation cannot be Compute Agnostic. In CVPR. [PDF](https://arxiv.org/pdf/2404.07177)

---

# Data Augmentation

1. T-MARS

2. Visual Data-Type Understanding does not emerge from scaling Vision-Language Models. In *ICLR*. [PDF](https://openreview.net/pdf?id=WyEdX2R4er)

3. 

---

## Synthetic data

### Captions

1. **Li et al.** (2022). BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. In *ICML*, 12888–12900. [PDF](https://arxiv.org/pdf/2201.12086.pdf)

2. **Li et al.** (2024). What If We Recaption Billions of Web Images with LLaMA-3? In *arXiv*.

3. **Sharifzadeh et al.** (2024). Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings. In *arXiv*.

4. **Liu et al.** (2024). CLIPS: An Enhanced CLIP Framework for Learning with Synthetic Captions. In *arXiv*.

5. **Yang et al.** (2023). ALIP: Adaptive Language-Image Pre-training with Synthetic Caption. In *ICCV*, 2910–2919. [PDF](https://arxiv.org/pdf/2310.07699.pdf)

6. **Fang et al.** (2024). VILA$^2$: VILA Augmented VILA. In *arXiv*.

7. **Fan et al.** (2023). Improving CLIP Training with Language Rewrites. In *NeurIPS*.

8. **Hammoud et al.** (2024). SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training? In *CVPR 2024 Synthetic Data for Computer Vision Workshop*. [PDF](https://arxiv.org/pdf/2402.01832.pdf)

9. **Lai et al.** (2025). VeCLIP: Improving CLIP Training via Visual-Enriched Captions. In *Lecture Notes in Computer Science*, 111–127. [PDF](https://arxiv.org/pdf/2310.07699.pdf)

10. **Nguyen et al.** (2023). Improving Multimodal Datasets with Image Captioning. In *NeurIPS*.

11. **Rotstein et al.** (2024). FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions. In *WACV*, 5677–5688. [PDF](https://arxiv.org/pdf/2402.01832.pdf)

12. **Santurkar et al.** (2023). Is a Caption Worth a Thousand Images? A Study on Representation Learning. In *ICLR*.

13. **Yu et al.** (2024). CapsFusion: Rethinking Image-Text Data at Scale. In *CVPR*.

14. **Zhu et al.** (2024). ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions. In *TMLR*. [PDF](https://arxiv.org/pdf/2303.06594.pdf)

### Images

1. SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models. In *ArXiV*. [PDF]()

2. **Sharifzadeh et al.** (2024). Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings. In *arXiv*.

3. **Hammoud et al.** (2024). SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?. In *CVPR Workshop*. [PDF](https://arxiv.org/pdf/2402.01832)

## Scaling

1. **Jia et al.** (2021). Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In *ICML*.

2. **Pham et al.** (2023). Combined scaling for zero-shot transfer learning. In *Neurocomputing*. [PDF](https://doi.org/10.1016/j.neucom.2023.126658)

## Generalization

1. **Prasanna et al. (2024). Does CLIP’s generalization performance mainly stem from high train-test similarity ? In *ICLR*. [PDF](https://openreview.net/pdf?id=tnBaiidobu)


## Evaluation

1. CLIPScore

2. **Elissa et al.** (2024). ContextRef: Evaluating Referenceless Metrics for Image Description Generation. In *ICLR*. [PDF](https://openreview.net/pdf?id=j0ZvKSNZiP)

## Multilinguality
